"""Report generation module for cancer analysis results."""

import json
import os
from datetime import datetime, timezone

import numpy as np

from cancer_agent.utils import get_logger

log = get_logger(__name__)


class Reporter:
    """Generates structured reports from the full analysis pipeline."""

    def generate(self, dataset_metadata: dict, eda_report: dict,
                 preprocessing_info: dict, training_results: dict,
                 evaluation_results: dict) -> dict:
        """
        Compile all results into a final structured report.
        """
        report = {
            "report_metadata": {
                "generated_at": datetime.now(timezone.utc).isoformat(),
                "agent_version": "0.1.0",
                "disclaimer": (
                    "This report is generated by an ML research tool for "
                    "educational and research purposes only. It does NOT "
                    "constitute medical advice or diagnosis."
                ),
            },
            "dataset": dataset_metadata,
            "exploratory_analysis": {
                "class_balance": eda_report.get("class_balance", {}),
                "top_discriminative_features": eda_report.get(
                    "top_discriminative_features", []
                ),
                "correlation_summary": eda_report.get("feature_correlations", {}),
                "outlier_summary": eda_report.get("outlier_summary", {}),
            },
            "preprocessing": preprocessing_info,
            "model_comparison": self._model_comparison_table(training_results),
            "evaluation": {
                "best_model": evaluation_results["best_model_name"],
                "best_metrics": evaluation_results["best_metrics"],
                "all_models": {
                    name: {k: v for k, v in metrics.items() if k != "top_features"}
                    for name, metrics in evaluation_results["evaluations"].items()
                },
            },
            "feature_importance": self._aggregate_feature_importance(
                evaluation_results
            ),
            "recommendations": self._generate_recommendations(
                eda_report, evaluation_results
            ),
        }

        return report

    def print_summary(self, report: dict) -> str:
        """Format the report as a human-readable text summary."""
        lines = []
        lines.append("=" * 70)
        lines.append("  AUTONOMOUS CANCER RESEARCH ML AGENT - ANALYSIS REPORT")
        lines.append("=" * 70)
        lines.append("")

        # Disclaimer
        lines.append("DISCLAIMER: " + report["report_metadata"]["disclaimer"])
        lines.append("")

        # Dataset info
        ds = report["dataset"]
        lines.append(f"Dataset: {ds['name']}")
        lines.append(f"Samples: {ds['n_samples']}  |  Features: {ds['n_features']}")
        lines.append(f"Task: {ds['task']}")
        lines.append(f"Class distribution: {ds['class_distribution']}")
        lines.append("")

        # EDA highlights
        eda = report["exploratory_analysis"]
        cb = eda.get("class_balance", {})
        lines.append(f"Class balance: {cb.get('status', 'N/A')} "
                      f"(ratio={cb.get('imbalance_ratio', 'N/A')})")

        top_feats = eda.get("top_discriminative_features", [])
        if top_feats:
            lines.append("Top discriminative features:")
            for i, f in enumerate(top_feats[:5]):
                lines.append(
                    f"  {i+1}. {f['feature']} "
                    f"(Cohen's d={f['effect_size_cohens_d']}, p={f['p_value']:.2e})"
                )
        lines.append("")

        # Model comparison
        lines.append("-" * 70)
        lines.append("MODEL COMPARISON (Cross-Validation)")
        lines.append("-" * 70)
        header = f"{'Model':<25} {'CV Acc':>10} {'Std':>10} {'Train Acc':>10}"
        lines.append(header)
        for m in report["model_comparison"]:
            lines.append(
                f"{m['name']:<25} {m['cv_accuracy_mean']:>10.4f} "
                f"{m['cv_accuracy_std']:>10.4f} {m['train_accuracy']:>10.4f}"
            )
        lines.append("")

        # Test evaluation
        lines.append("-" * 70)
        lines.append("TEST SET EVALUATION")
        lines.append("-" * 70)
        eval_data = report["evaluation"]
        header = f"{'Model':<25} {'Acc':>8} {'Prec':>8} {'Recall':>8} {'F1':>8} {'AUC':>8} {'MCC':>8}"
        lines.append(header)
        for name, m in eval_data["all_models"].items():
            lines.append(
                f"{name:<25} {m['accuracy']:>8.4f} {m['precision']:>8.4f} "
                f"{m['recall']:>8.4f} {m['f1']:>8.4f} "
                f"{m.get('roc_auc', 'N/A'):>8} {m['mcc']:>8.4f}"
            )
        lines.append("")

        best = eval_data["best_model"]
        bm = eval_data["best_metrics"]
        lines.append(f">>> Best model: {best}")
        lines.append(f"    Accuracy={bm['accuracy']}, F1={bm['f1']}, "
                      f"AUC={bm.get('roc_auc', 'N/A')}, MCC={bm['mcc']}")
        lines.append("")

        # Feature importance
        fi = report.get("feature_importance", [])
        if fi:
            lines.append("-" * 70)
            lines.append("TOP PREDICTIVE FEATURES (aggregated)")
            lines.append("-" * 70)
            for i, f in enumerate(fi[:10]):
                lines.append(f"  {i+1}. {f['feature']} (importance={f['importance']:.6f})")
            lines.append("")

        # Recommendations
        recs = report.get("recommendations", [])
        if recs:
            lines.append("-" * 70)
            lines.append("AGENT RECOMMENDATIONS")
            lines.append("-" * 70)
            for r in recs:
                lines.append(f"  [{r['priority']}] {r['recommendation']}")
            lines.append("")

        lines.append("=" * 70)
        lines.append("END OF REPORT")
        lines.append("=" * 70)

        summary = "\n".join(lines)
        log.info("Report generated (%d lines)", len(lines))
        return summary

    def save_json(self, report: dict, path: str) -> str:
        """Save the report as a JSON file."""
        serializable = self._make_serializable(report)
        os.makedirs(os.path.dirname(path) if os.path.dirname(path) else ".", exist_ok=True)
        with open(path, "w") as f:
            json.dump(serializable, f, indent=2, default=str)
        log.info("Report saved to %s", path)
        return path

    def _model_comparison_table(self, training_results: dict) -> list[dict]:
        """Build a sorted model comparison list."""
        table = []
        for entry in training_results["results"]:
            table.append({
                "name": entry["name"],
                "cv_accuracy_mean": entry["cv_accuracy_mean"],
                "cv_accuracy_std": entry["cv_accuracy_std"],
                "train_accuracy": entry["train_accuracy"],
                "cv_time_seconds": entry["cv_time_seconds"],
            })
        return table

    def _aggregate_feature_importance(self, evaluation_results: dict) -> list[dict]:
        """Aggregate feature importance across all models that support it."""
        from collections import defaultdict
        scores = defaultdict(list)

        for name, metrics in evaluation_results["evaluations"].items():
            for feat_entry in metrics.get("top_features", []):
                scores[feat_entry["feature"]].append(feat_entry["importance"])

        if not scores:
            return []

        aggregated = [
            {"feature": feat, "importance": round(np.mean(vals), 6)}
            for feat, vals in scores.items()
        ]
        aggregated.sort(key=lambda x: x["importance"], reverse=True)
        return aggregated

    def _generate_recommendations(self, eda_report: dict,
                                   evaluation_results: dict) -> list[dict]:
        """Generate actionable recommendations based on analysis results."""
        recs = []

        # Class balance recommendations
        cb = eda_report.get("class_balance", {})
        if cb.get("status") == "severe_imbalance":
            recs.append({
                "priority": "HIGH",
                "recommendation": (
                    "Severe class imbalance detected. Consider SMOTE oversampling, "
                    "class weights, or undersampling strategies."
                ),
            })
        elif cb.get("status") == "moderate_imbalance":
            recs.append({
                "priority": "MEDIUM",
                "recommendation": (
                    "Moderate class imbalance detected. Consider using class_weight='balanced' "
                    "in model training."
                ),
            })

        # Correlation recommendations
        corr = eda_report.get("feature_correlations", {})
        n_high = corr.get("n_highly_correlated", 0)
        if n_high > 5:
            recs.append({
                "priority": "MEDIUM",
                "recommendation": (
                    f"{n_high} highly correlated feature pairs found. "
                    "Consider PCA or feature selection to reduce multicollinearity."
                ),
            })

        # Model performance recommendations
        best_metrics = evaluation_results.get("best_metrics", {})
        best_f1 = best_metrics.get("f1", 0)
        if best_f1 < 0.9:
            recs.append({
                "priority": "MEDIUM",
                "recommendation": (
                    f"Best F1 score is {best_f1}. Consider hyperparameter tuning, "
                    "feature engineering, or ensemble methods to improve performance."
                ),
            })
        if best_f1 >= 0.95:
            recs.append({
                "priority": "INFO",
                "recommendation": (
                    f"Excellent F1={best_f1}. Verify results are not due to data leakage. "
                    "Consider external validation on an independent dataset."
                ),
            })

        # General
        recs.append({
            "priority": "INFO",
            "recommendation": (
                "All results are from a single train/test split. "
                "For publication-quality results, use nested cross-validation."
            ),
        })

        return recs

    def _make_serializable(self, obj):
        """Recursively convert numpy types to Python native types."""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [self._make_serializable(v) for v in obj]
        if isinstance(obj, (np.integer,)):
            return int(obj)
        if isinstance(obj, (np.floating,)):
            return float(obj)
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj
